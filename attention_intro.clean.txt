Recurrent neural networks, long short-term memory gated recurrent neural networks particular, been firmly established state art approaches sequence modeling transduction problems such language modeling machine translation. Numerous efforts since continued push boundaries recurrent language models encoder-decoder architectures. Recurrent models typically factor computation along symbol positions input output sequences. Aligning positions steps computation time, they generate sequence hidden states ht, function previous hidden state htâˆ’1 input position t. inherently sequential nature precludes parallelization within training examples, becomes critical longer sequence lengths, memory constraints limit batching across examples. Recent work has achieved significant improvements computational efficiency through factorization tricks conditional computation, while also improving model performance case latter. fundamental constraint sequential computation, however, remains. Attention mechanisms become integral part compelling sequence modeling transduction models various tasks, allowing modeling dependencies without regard their distance input output sequences. all few cases, however, such attention mechanisms are used conjunction recurrent network. work propose Transformer, model architecture eschewing recurrence instead relying entirely attention mechanism draw global dependencies between input output. Transformer allows significantly more parallelization can reach new state art translation quality after being trained little twelve hours eight P100 GPUs.